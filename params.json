{"name":"Wells Fargo Analytics","tagline":"","body":"#**About**\r\nWelcome! I'm Jeniffer Soto, student of DATA 101 from the College of Charleston. This space is dedicated to provide information about the Wells Fargo Analytics competition. Feel free to navigate the following sections, which provide related information on the Deliverables. If you have any questions, please contact me at sotoperezj@g.cofc.edu\r\n\r\n#**Challenge**\r\nDialogues on social media can provide tremendous insight into the behaviors, desires, pains, and thoughts of consumers. We'd like your help in developing a repeatable process that identifies, classifies, and extracts the underlying drivers of consumer financial conversations and comments in social media data. \r\n\r\nProvide an analytic report of 1,500 words or less that is structured as outlined below, utilizing the dataset.txt provided by Wells Fargo.\r\n\r\n####**Question 1** - What financial topics do consumers discuss on social media and what caused the consumers to post about this topic? \r\n\r\n###**Deliverable A** - Describe your Approach and Methodology.\r\n###**Preparing the data**\r\nTo obtain only the data that's useful out of the 220,377 observations, we upload the data set into the R program and proceed to pre-process the data. When uploaded in R, inspect the type of data dealt with, which in this competition is comments from facebook posts and tweets.  The pre-process consists of getting rid of unnecessary spaces, words, and facebook posts, or tweets. During this phase we select a sample size, in this case of 10,000 observations, to reduce the number of observations we have to work with while staying statistically sound for predictions.  \r\n\r\nThe team indexed each bank as, BankA, BankB, Bank C, and Bank D. Then we tested the data to find positive and negative sentiment words as well as high frequency words. From these, we can have an idea of the main topics mentioned. The results can be downloaded as an excel file during this step. In this case the excel files created were positive.txt.csv and negative.txt.csv.\r\n\r\nAfter categorizing the csv's files into \"relevant\" and non-\"relevant\" to positive or negative comments, we can upload the csv's back to R and train on them for classification purposes. From here, many approaches can be applied, for example clustering approach. \r\n\r\n![Flow Chart](https://raw.githubusercontent.com/jennsoto/wells_fargo_analytics/d37e7cb42a5749583d23ad164b0b4f3a91fe95cd/images/flow%20chart.png)\r\n\r\n###**Deliverable B** - Discuss the data and its relationship to social conversation drivers.\r\nTo find the social conversation drivers, indexed the data. After analysing the positive and negative comments we recognized conversation drivers like atm fee, customer service, Wells Fargo donation accounts, and account issues. The data provides an idea of how many banking conversations can be considered as social drivers. \r\n\r\n###**Deliverable C** - Document your code and reference the analytic process flow-diagram from deliverable A. \r\n####Code\r\n```R\r\n# Load data set, in my case ('df.Rda')\r\nload(\"/Users/Jenn/Desktop/Analytic_Competition/df.Rda\")\r\ndf$FullText = as.character(df$FullText)\r\n\r\n# Grab just the texts, so you can load them in the Corpus\r\ndf.texts = as.data.frame(df[,ncol(df)])\r\ncolnames(df.texts) = 'FullText'\r\n\r\n# Remove non-ascii characters\r\ndf.texts.clean = as.data.frame(iconv(df.texts$FullText, \"latin1\", \r\n                                     \"ASCII\", sub=\"\"))\r\ncolnames(df.texts.clean) = 'FullText'\r\n\r\ndf$FullText = df.texts.clean$FullText\r\n\r\n# To test on 10000 samples using df.10000\r\nidx.10000 = sample(1:nrow(df),10000)\r\ndf.10000 = df[idx.10000,]\r\n\r\ndf.entire = df\r\ndf = df.10000\r\n\r\n# Load in corpus form using the tm library\r\nlibrary(tm) \r\ndocs <- Corpus(DataframeSource(as.data.frame(df[,6])))   \r\n\r\n# Perform pre-processing\r\ndocs <- tm_map(docs, PlainTextDocument)\r\ndocs <- tm_map(docs, removePunctuation)\r\ndocs <- tm_map(docs, stripWhitespace)\r\ndocs <- tm_map(docs, removeWords,c(\"Name\",\"and\",\"for\")) \r\ndocs <- tm_map(docs, removeWords, stopwords(\"english\"))\r\ndocs <- tm_map(docs, removeWords, stopwords(kind=\"SMART\"))\r\n\r\nsave.image('docs.preprocessed.Rda')\r\nload('docs.preprocessed.Rda')\r\n\r\n# Documents containing each bank\r\nbankA.idx = which(sapply(df$FullText,function(x) grepl(\"BankA\",x)))\r\nbankB.idx = which(sapply(df$FullText,function(x) grepl(\"BankB\",x)))\r\nbankC.idx = which(sapply(df$FullText,function(x) grepl(\"BankC\",x)))\r\nbankD.idx = which(sapply(df$FullText,function(x) grepl(\"BankD\",x)))\r\n\r\ndf$BankID = vector(mode=\"numeric\", length = nrow(df))\r\ndf$BankID[bankA.idx] = \"BankA\"\r\ndf$BankID[bankB.idx] = \"BankB\"\r\ndf$BankID[bankC.idx] = \"BankC\"\r\ndf$BankID[bankD.idx] = \"BankD\"\r\n\r\nbankA.docs = docs[bankA.idx]\r\nbankB.docs = docs[bankB.idx]\r\nbankC.docs = docs[bankC.idx]\r\nbankD.docs = docs[bankD.idx]\r\n\r\nsummary(docs)\r\n\r\n## Repeat these processes for every bank\r\n## Create document term matrix\r\ndtm <- DocumentTermMatrix(docs[bankA.idx])\r\n\r\n## Transpose this matrix\r\ntdm <- TermDocumentMatrix(docs[bankA.idx])\r\n\r\n## Remove sparse terms\r\ndtm = removeSparseTerms(dtm, 0.98)\r\n\r\n## Organize terms by frequency\r\nfindFreqTerms(dtm,50)\r\nfreq <- colSums(as.matrix(dtm))  \r\nord <- order(freq)   \r\nfreq[head(ord)]  \r\nfreq[tail(ord)]\r\n\r\nwf <- data.frame(word=names(freq), freq=freq)   \r\nhead(wf)\r\n\r\n## Plot word frequencies\r\nlibrary(ggplot2)   \r\np <- ggplot(subset(wf, freq>100), aes(word, freq))    \r\np <- p + geom_bar(stat=\"identity\")   \r\np <- p + theme(axis.text.x=element_text(angle=45, hjust=1))   \r\np\r\n\r\n## To get a word cloud of the 100 most frequent words \r\nlibrary(wordcloud)\r\nset.seed(142)   \r\ndark2 <- brewer.pal(6, \"Dark2\")   \r\nwordcloud(names(freq), freq, max.words=25, rot.per=0.2, colors=dark2)\r\n\r\n# Sentiment analysis\r\npos <- scan('positive-words.txt',what='character',comment.char=';')\r\nneg <- scan('negative-words.txt',what='character',comment.char=';')\r\n\r\nscore.sentiment = function(sentences, pos.words, neg.words, .progress='none')\r\n{\r\n  require(plyr)\r\n  require(stringr)\r\n  \r\n  scores = laply(sentences, function(sentence, pos.words, neg.words) {\r\n    \r\n    # Clean up sentences with R's regex-driven global substitute, gsub():\r\n    sentence = gsub('[[:punct:]]', '', sentence)\r\n    sentence = gsub('[[:cntrl:]]', '', sentence)\r\n    sentence = gsub('\\\\d+', '', sentence)\r\n    sentence = tolower(sentence)\r\n    \r\n    # Split into words. You need the stringr package\r\n    word.list = str_split(sentence, '\\\\s+')\r\n    # Sometimes a list() is one level of hierarchy too much\r\n    words = unlist(word.list)\r\n    \r\n    # Compare words to positive & negative terms\r\n    pos.matches = match(words, pos.words)\r\n    neg.matches = match(words, neg.words)\r\n    \r\n    # match() returns the position of the matched term or NA\r\n    pos.matches = !is.na(pos.matches)\r\n    neg.matches = !is.na(neg.matches)\r\n    \r\n    # TRUE/FALSE will be treated as 1/0 by sum():\r\n    score = sum(pos.matches) - sum(neg.matches)\r\n    \r\n    return(score)\r\n  }, pos.words, neg.words, .progress=.progress )\r\n  \r\n  scores.df = data.frame(score=scores, text=sentences)\r\n  return(scores.df)\r\n}\r\n\r\n# Very positive and negative\r\ndf.sentiment = df[bankA.idx,]\r\n\r\nscores = score.sentiment(df.sentiment$FullText, pos, neg, .progress='text')\r\nscores$very.pos = as.numeric(scores$score >= 2)\r\nscores$very.neg = as.numeric(scores$score <= -2)\r\n\r\npos.tweets = which(scores$very.pos == 1)\r\nneg.tweets = which(scores$very.neg == 1)\r\nwrite.csv(df.sentiment[pos.tweets,],file='pos.texts.csv')##creates positive\r\nwrite.csv(df.sentiment[neg.tweets,],file='neg.texts.csv')##creates negative\r\n\r\n# Creating a Classifier\r\nload('df.Rda')\r\ndf$FullText = as.character(df$FullText)\r\npos.texts = read.csv('pos.texts.csv',header=T)\r\npos.texts$FullText = as.character(pos.texts$FullText)\r\ncolnames(pos.texts)\r\ndocs <- Corpus(DataframeSource(as.data.frame(pos.texts[,9])))   \r\ndocs <- tm_map(docs, PlainTextDocument)\r\n\r\ndtm <- DocumentTermMatrix(docs)\r\n\r\ntdm <- TermDocumentMatrix(docs)\r\n\r\nm = as.matrix(dtm)\r\n\r\ndf.classification = as.data.frame(m) #dataframe\r\ndf.classification$Relevant = pos.texts$Relevant\r\ndf.classification$Relevant\r\n\r\n# Grow a tree\r\nlibrary(rpart)\r\nfit<-rpart(Relevant ~ X + AutoID + Date + Year + Month + MediaType + FullText  \r\n           + BankID, data = pos.texts, method = \"class\")\r\nprintcp(fit) # display the results \r\nplotcp(fit) # visualize cross-validation results \r\nsummary(fit) # detailed summary of splits\r\n\r\n########### Clustering approach\r\ndtm = DocumentTermMatrix(docs)\r\ndtm = removeSparseTerms(dtm, 0.98)\r\nbag.of.words = as.matrix(dtm)\r\ndtm\r\n\r\nhc = hclust(dist(bag.of.words))\r\nhcd = as.dendrogram(hc)\r\ng246 = cutree(hc, k=c(2,4,6,20))\r\ngroup.idxs = list()\r\nn.groups = 20\r\ndtms = list()\r\ndtms.matrcies = list()\r\nfor (i in 1:n.groups) {\r\n  group.idxs[[i]] = which(g246[,\"20\"]==i)\r\n  dtms[[i]] = DocumentTermMatrix(docs[group.idxs[[i]]])\r\n  dtm.matrix = as.matrix(dtms[[i]])\r\n  print(paste0('Group,i'))\r\n  frequency = colSums(dtm.matrix)\r\n  frequency = sort(frequency, decreasing = TRUE)\r\n  print(head(frequency))\r\n}\r\n```\r\n####**Question #2** - Are the topics and “substance” consistent across the industry or are they isolated to individual banks?\r\n###**Deliverable D** -  Create a list of topics and substance you found.\r\n####Topics: \r\n* Account\r\n* Check\r\n* Customer Service \r\n* Credit\r\n* Grants \r\n* Management\r\n* ATM\r\n* Phone Calls\r\n\r\n####Substance:\r\n* Customer Attrition — Likes customer service/dislikes customer service\r\n\r\n###**Deliverable E** - Create a narrative of insights supported by the quantitative results (should include graphs or charts). \r\n\r\n####100 Most Frequent Words\r\n![100_most_frequent](https://raw.githubusercontent.com/jennsoto/wells_fargo_analytics/095790653456e1eed525680946f2e8d7cc9dd1bc/images/100.maxwords.png)\r\n\r\n![plot_100](https://raw.githubusercontent.com/jennsoto/wells_fargo_analytics/9822cf99ffd31429dd0d548032f050b109db6c83/images/plot100.png)\r\n\r\nThe most frequent words obtained served as a guide to understand what subjects customers are talking about. According to our results, customers are commenting about online services, phone calls, customer service, and bank accounts. \r\n\r\nAccording to the classification tree results, positive statements within a .5 relevancy are \"thanks,\" \"very,\" \"service,\" \"support,\" \"love,\" and \"thank.\" With more work on the csv's files, we could utilize this method to obtain only the positive or negative comments.\r\n\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}