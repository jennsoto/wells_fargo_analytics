{"name":"Wells Fargo Analytics","tagline":"","body":"#**About**\r\nWelcome! I'm Jeniffer Soto, student of DATA 101 from the College of Charleston. This space is dedicated to provide information about the Wells Fargo Analytics competition. Feel free to navigate the following sections, which provide related information on the Deliverables. If you have any questions, please contact me at sotoperezj@g.cofc.edu\r\n\r\n#**Challenge**\r\nDialogues on social media can provide tremendous insight into the behaviors, desires, pains, and thoughts of consumers. We'd like your help in developing a repeatable process that identifies, classifies, and extracts the underlying drivers of consumer financial conversations and comments in social media data. \r\n\r\nProvide an analytic report of 1,500 words or less that is structured as outlined below, utilizing the dataset.txt provided by Wells Fargo.\r\n\r\n\r\n![social_media](https://raw.githubusercontent.com/jennsoto/wells_fargo_analytics/050e56f82eea7358e059f53b815887b4d0e31bc9/images/social_media_freak.jpg)\r\n\r\n\r\n####**Question 1** - What financial topics do consumers discuss on social media and what caused the consumers to post about this topic? \r\n\r\n###**Deliverable A** - Describe your Approach and Methodology.\r\n###**Preparing the data**\r\nTo obtain only the data that's useful out of the 220,377 observations, we upload the data set into the R program and proceed to pre-process the data. When uploaded in R, we inspected the type of data dealt with, which in this competition is comments from facebook posts, and tweets.  The pre-process consists of getting rid of unnecessary spaces, words, and facebook posts, or tweets. During this phase we selected a sample size of 10,000 observations to reduce the number of observations we have to work with when categorizing the comments as relevant or non-relevant.  \r\n\r\nThe team indexed each bank as, BankA, BankB, Bank C, and Bank D. Then we tested the data to find positive and negative sentiment words as well as high frequency words. From these, we can have an idea of the main topics mentioned. The results can be downloaded as an excel file during this step. For this competition, the excel files created were positive.txt.csv and negative.txt.csv.\r\n\r\nAfter categorizing the csv's files into \"relevant\" and non-\"relevant\" to positive or negative comments, we can upload the csv's back to R and train on them for classification purposes. From here, many approaches can be applied, for example clustering approach. \r\n\r\n![Flow Chart](https://raw.githubusercontent.com/jennsoto/wells_fargo_analytics/d37e7cb42a5749583d23ad164b0b4f3a91fe95cd/images/flow%20chart.png)\r\n\r\n###**Deliverable B** - Discuss the data and its relationship to social conversation drivers.\r\nTo find the social conversation drivers, indexed the data. After analysing the positive and negative comments we recognized conversation drivers like atm fee, customer service, Wells Fargo donation accounts, and account issues. The data provides an idea of how many banking conversations can be considered as social drivers. \r\n\r\n###**Deliverable C** - Document your code and reference the analytic process flow-diagram from deliverable A. \r\n####Code\r\n```R\r\n# Load data set, in my case ('df.Rda')\r\nload(\"/Users/Jenn/Desktop/Analytic_Competition/df.Rda\")\r\ndf$FullText = as.character(df$FullText)\r\n\r\n# Grab just the texts, so you can load them in the Corpus\r\ndf.texts = as.data.frame(df[,ncol(df)])\r\ncolnames(df.texts) = 'FullText'\r\n\r\n# Remove non-ascii characters\r\ndf.texts.clean = as.data.frame(iconv(df.texts$FullText, \"latin1\", \r\n                                     \"ASCII\", sub=\"\"))\r\ncolnames(df.texts.clean) = 'FullText'\r\n\r\ndf$FullText = df.texts.clean$FullText\r\n\r\n# To test on 10000 samples using df.10000\r\nidx.10000 = sample(1:nrow(df),10000)\r\ndf.10000 = df[idx.10000,]\r\n\r\ndf.entire = df\r\ndf = df.10000\r\n\r\n# Load in corpus form using the tm library\r\nlibrary(tm) \r\ndocs <- Corpus(DataframeSource(as.data.frame(df[,6])))   \r\n\r\n# Perform pre-processing\r\ndocs <- tm_map(docs, PlainTextDocument)\r\ndocs <- tm_map(docs, removePunctuation)\r\ndocs <- tm_map(docs, stripWhitespace)\r\ndocs <- tm_map(docs, removeWords,c(\"Name\",\"and\",\"for\")) \r\ndocs <- tm_map(docs, removeWords, stopwords(\"english\"))\r\ndocs <- tm_map(docs, removeWords, stopwords(kind=\"SMART\"))\r\n\r\nsave.image('docs.preprocessed.Rda')\r\nload('docs.preprocessed.Rda')\r\n\r\n# Documents containing each bank\r\nbankA.idx = which(sapply(df$FullText,function(x) grepl(\"BankA\",x)))\r\nbankB.idx = which(sapply(df$FullText,function(x) grepl(\"BankB\",x)))\r\nbankC.idx = which(sapply(df$FullText,function(x) grepl(\"BankC\",x)))\r\nbankD.idx = which(sapply(df$FullText,function(x) grepl(\"BankD\",x)))\r\n\r\ndf$BankID = vector(mode=\"numeric\", length = nrow(df))\r\ndf$BankID[bankA.idx] = \"BankA\"\r\ndf$BankID[bankB.idx] = \"BankB\"\r\ndf$BankID[bankC.idx] = \"BankC\"\r\ndf$BankID[bankD.idx] = \"BankD\"\r\n\r\nbankA.docs = docs[bankA.idx]\r\nbankB.docs = docs[bankB.idx]\r\nbankC.docs = docs[bankC.idx]\r\nbankD.docs = docs[bankD.idx]\r\n\r\nsummary(docs)\r\n\r\n## Repeat these processes for every bank\r\n## Create document term matrix\r\ndtm <- DocumentTermMatrix(docs[bankA.idx])\r\n\r\n## Transpose this matrix\r\ntdm <- TermDocumentMatrix(docs[bankA.idx])\r\n\r\n## Remove sparse terms\r\ndtm = removeSparseTerms(dtm, 0.98)\r\n\r\n## Organize terms by frequency\r\nfindFreqTerms(dtm,50)\r\nfreq <- colSums(as.matrix(dtm))  \r\nord <- order(freq)   \r\nfreq[head(ord)]  \r\nfreq[tail(ord)]\r\n\r\nwf <- data.frame(word=names(freq), freq=freq)   \r\nhead(wf)\r\n\r\n## Plot word frequencies\r\nlibrary(ggplot2)   \r\np <- ggplot(subset(wf, freq>100), aes(word, freq))    \r\np <- p + geom_bar(stat=\"identity\")   \r\np <- p + theme(axis.text.x=element_text(angle=45, hjust=1))   \r\np\r\n\r\n## To get a word cloud of the 100 most frequent words \r\nlibrary(wordcloud)\r\nset.seed(142)   \r\ndark2 <- brewer.pal(6, \"Dark2\")   \r\nwordcloud(names(freq), freq, max.words=25, rot.per=0.2, colors=dark2)\r\n\r\n# Sentiment analysis\r\npos <- scan('positive-words.txt',what='character',comment.char=';')\r\nneg <- scan('negative-words.txt',what='character',comment.char=';')\r\n\r\nscore.sentiment = function(sentences, pos.words, neg.words, .progress='none')\r\n{\r\n  require(plyr)\r\n  require(stringr)\r\n  \r\n  scores = laply(sentences, function(sentence, pos.words, neg.words) {\r\n    \r\n    # Clean up sentences with R's regex-driven global substitute, gsub():\r\n    sentence = gsub('[[:punct:]]', '', sentence)\r\n    sentence = gsub('[[:cntrl:]]', '', sentence)\r\n    sentence = gsub('\\\\d+', '', sentence)\r\n    sentence = tolower(sentence)\r\n    \r\n    # Split into words. You need the stringr package\r\n    word.list = str_split(sentence, '\\\\s+')\r\n    # Sometimes a list() is one level of hierarchy too much\r\n    words = unlist(word.list)\r\n    \r\n    # Compare words to positive & negative terms\r\n    pos.matches = match(words, pos.words)\r\n    neg.matches = match(words, neg.words)\r\n    \r\n    # match() returns the position of the matched term or NA\r\n    pos.matches = !is.na(pos.matches)\r\n    neg.matches = !is.na(neg.matches)\r\n    \r\n    # TRUE/FALSE will be treated as 1/0 by sum():\r\n    score = sum(pos.matches) - sum(neg.matches)\r\n    \r\n    return(score)\r\n  }, pos.words, neg.words, .progress=.progress )\r\n  \r\n  scores.df = data.frame(score=scores, text=sentences)\r\n  return(scores.df)\r\n}\r\n\r\n# Very positive and negative\r\ndf.sentiment = df[bankA.idx,]\r\n\r\nscores = score.sentiment(df.sentiment$FullText, pos, neg, .progress='text')\r\nscores$very.pos = as.numeric(scores$score >= 2)\r\nscores$very.neg = as.numeric(scores$score <= -2)\r\n\r\npos.tweets = which(scores$very.pos == 1)\r\nneg.tweets = which(scores$very.neg == 1)\r\nwrite.csv(df.sentiment[pos.tweets,],file='pos.texts.csv')##creates positive\r\nwrite.csv(df.sentiment[neg.tweets,],file='neg.texts.csv')##creates negative\r\n\r\n# Creating a classifier for pos.texts\r\nload('df.Rda')\r\ndf$FullText = as.character(df$FullText)\r\npos.texts = read.csv('pos.texts.csv',header=T)\r\npos.texts$FullText = as.character(pos.texts$FullText)\r\ncolnames(pos.texts)\r\ndocs <- Corpus(DataframeSource(as.data.frame(pos.texts[,9])))   \r\ndocs <- tm_map(docs, PlainTextDocument)\r\n\r\ndtm <- DocumentTermMatrix(docs)\r\n\r\ntdm <- TermDocumentMatrix(docs)\r\n\r\nm = as.matrix(dtm)\r\n\r\ndf.classification = as.data.frame(m) #dataframe\r\ndf.classification$Relevant = pos.texts$Relevant\r\ndf.classification$Relevant\r\n\r\n# Grow a tree\r\nlibrary(rpart)\r\nfit<-rpart(Relevant ~ FullText + BankID, data = pos.texts, method = \"class\")\r\nprintcp(fit) # display the results \r\nplotcp(fit) # visualize cross-validation results \r\nsummary(fit) # detailed summary of splits\r\n\r\n# Creating a classifier for neg.texts\r\nload('df.Rda')\r\ndf$FullText = as.character(df$FullText)\r\nneg.texts = read.csv('neg.texts.csv',header=T)\r\nneg.texts$FullText = as.character(neg.texts.texts$FullText)\r\ncolnames(neg.texts)\r\ndocs <- Corpus(DataframeSource(as.data.frame(neg.texts[,9])))   \r\ndocs <- tm_map(docs, PlainTextDocument)\r\n\r\ndtm <- DocumentTermMatrix(docs)\r\n\r\ntdm <- TermDocumentMatrix(docs)\r\n\r\nm = as.matrix(dtm)\r\n\r\ndf.classification = as.data.frame(m) #dataframe\r\ndf.classification$Relevant = neg.texts$Relevant\r\ndf.classification$Relevant\r\ncolnames(neg.texts)\r\n\r\n# Grow a tree\r\nlibrary(rpart)\r\nfit<-rpart(Relevant ~ FullText + BankID, data = neg.texts, method = \"class\")\r\nprintcp(fit) # display the results \r\nplotcp(fit) # visualize cross-validation results \r\nsummary(fit) # detailed summary of splits\r\n\r\n## Cluster Dendrogram\r\n# Useless words\r\ndocs <- tm_map(docs, removeWords,c(\"Name\", \"and\",\"for\", \"name\", \"this\", \"are\",\"from\", \"just\", \"get\", \"ret_twit\", \"name_resp\", \"twit_hndl\", \"twit_handl:\", \"twit_hndl_banka\",\"ly/\")) \r\ndocs <- tm_map(docs, removeWords, stopwords(\"english\"))\r\ndocs <- tm_map(docs, removeWords, stopwords(kind=\"SMART\"))\r\n\r\ndtm <- DocumentTermMatrix(docs)\r\ntdm <- TermDocumentMatrix(docs)\r\n\r\n# remove sparse terms\r\ntdm2 <- removeSparseTerms(tdm, sparse = 0.97)\r\nm2 <- as.matrix(tdm2)\r\n# cluster terms\r\ndistMatrix <- dist(scale(m2))\r\nfit <- hclust(distMatrix, method = \"ward.D\")\r\n\r\nplot(fit)\r\nrect.hclust(fit, k = 6)\r\n\r\n##############\r\n```\r\n\r\n####**Question #2** - Are the topics and “substance” consistent across the industry or are they isolated to individual banks?\r\n###**Deliverable D** -  Create a list of topics and substance you found.\r\n####Topics: \r\n* Account\r\n* Check\r\n* Customer Service \r\n* Credit\r\n* Grants \r\n* Management\r\n* ATM\r\n* Phone Calls\r\n\r\n####Substance:\r\n* Customer Attrition — Likes customer service/dislikes customer service\r\n\r\nIn general, the topics and substance are consistent across the banking industry.\r\n\r\n###**Deliverable E** - Create a narrative of insights supported by the quantitative results (should include graphs or charts). \r\n\r\n####100 Most Frequent Words\r\n![100_most_frequent](https://raw.githubusercontent.com/jennsoto/wells_fargo_analytics/095790653456e1eed525680946f2e8d7cc9dd1bc/images/100.maxwords.png)\r\n\r\n![plot_100](https://raw.githubusercontent.com/jennsoto/wells_fargo_analytics/9822cf99ffd31429dd0d548032f050b109db6c83/images/plot100.png)\r\n\r\nThe most frequent words obtained served as a guide to understand what subjects customers are talking about. According to our results, customers are commenting about online services, phone calls, customer service, and bank accounts. \r\n\r\n\r\n![banking](https://raw.githubusercontent.com/jennsoto/wells_fargo_analytics/050e56f82eea7358e059f53b815887b4d0e31bc9/images/mobile-banking.jpg)\r\n\r\n\r\n####Classification Tree\r\n\r\n```R\r\n# Output\r\nNode number 1: 301 observations\r\n  predicted class=N  expected loss=0.4551495  P(node) =1\r\n    class counts:   164   137\r\n   probabilities: 0.545 0.455 \r\n\r\nNode number 1: 152 observations,    complexity param=1\r\n  predicted class=Y  expected loss=0.3223684  P(node) =1\r\n    class counts:    49   103\r\n   probabilities: 0.322 0.678 \r\n```\r\n\r\n![classification_tree](https://raw.githubusercontent.com/jennsoto/wells_fargo_analytics/0c1e1a57e68434d3f06d16b7808de094ec7a0e41/images/classification%20tree.png)\r\n\r\nAccording to the classification tree results, positive statements within a .5 relevancy are \"thanks,\" \"very,\" \"service,\" \"support,\" \"love,\" and \"thank.\" With more work on the csv's files, we could utilize this method to obtain only the positive or negative comments.\r\nOut of the 301 comments in the pos.txt csv file, 137 were actual positive comments towards one of the banks. We found 152 observations in the neg.texts csv file, 103 of those were negative comments about one of the banks.\r\n\r\n####Cluster Dendrogram\r\n![cluster_dendrogram](https://raw.githubusercontent.com/jennsoto/wells_fargo_analytics/cad45dbc9a8d54d0bf8062e8591d4dae1c930de0/images/cluster%20dendogram.png)\r\n\r\nWith a cluster dendrogram we can get close to predicting how words relate to each other, and how often. In the example above, we could predict that \"banka\", \"bankb,\" and \"bankd\" are the most mentioned on the internet. Another prediction from this dendrogram shows customers commenting about business bank accounts, and banka. ","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}