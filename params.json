{"name":"Wells Fargo Analytics","tagline":"","body":"#**About**\r\nWelcome! I'm Jeniffer Soto, student of DATA 101 from the College of Charleston. This space is dedicated to provide information about the Wells Fargo Analytics competition. Feel free to navigate the following sections, which provide related information on the Deliverables. If you have any questions, please contact me at sotoperezj@g.cofc.edu\r\n\r\n#**Challenge**\r\nDialogues on social media can provide tremendous insight into the behaviors, desires, pains, and thoughts of consumers. We'd like your help in developing a repeatable process that identifies, classifies, and extracts the underlying drivers of consumer financial conversations and comments in social media data. \r\n\r\nProvide an analytic report of 1,500 words or less that is structured as outlined below, utilizing the dataset.txt provided by Wells Fargo.\r\n\r\n####Question 1-**What financial topics do consumers discuss on social media and what caused the consumers to post about this topic?** \r\n\r\n##**Deliverable A - Describe your Approach and Methodology.**\r\n###Preparing the data\r\nTo obtain only the data that's useful out of the 220,377 observations, we upload the data set into the R program and proceed to pre-process the data. When uploaded in R, inspect the type of data dealt with, which in this competition is comments from facebook posts and tweets.  The pre-process consists of getting rid of unnecessary spaces, words, and facebook posts, or tweets. During this phase we select a sample size, in this case of 10,000 observations, to reduce the number of observations we have to work with while staying statistically sound for predictions.  \r\n\r\n####Code\r\n```R\r\n# Load data set, in my case ('df.Rda')\r\nload(\"/Users/Jenn/Desktop/Analytic_Competition/df.Rda\")\r\ndf$FullText = as.character(df$FullText)\r\n\r\n# Grab just the texts, so you can load them in the Corpus\r\ndf.texts = as.data.frame(df[,ncol(df)])\r\ncolnames(df.texts) = 'FullText'\r\n\r\n# Remove non-ascii characters\r\ndf.texts.clean = as.data.frame(iconv(df.texts$FullText, \"latin1\", \"ASCII\", sub=\"\"))\r\ncolnames(df.texts.clean) = 'FullText'\r\n\r\ndf$FullText = df.texts.clean$FullText\r\n\r\n# If you want to test on just 10000 records using df.10000 created below\r\nidx.10000 = sample(1:nrow(df),10000)\r\ndf.10000 = df[idx.10000,]\r\n\r\ndf.entire = df\r\ndf = df.10000\r\n\r\n# Load in corpus form using the tm library\r\nlibrary(tm) \r\ndocs <- Corpus(DataframeSource(as.data.frame(df[,6])))   \r\n\r\n# Perform pre-processing\r\ndocs <- tm_map(docs, PlainTextDocument)\r\ndocs <- tm_map(docs, removePunctuation)\r\ndocs <- tm_map(docs, stripWhitespace)\r\ndocs <- tm_map(docs, removeWords,c(\"Name\",\"and\",\"for\")) \r\ndocs <- tm_map(docs, removeWords, stopwords(\"english\"))\r\ndocs <- tm_map(docs, removeWords, stopwords(kind=\"SMART\"))\r\n\r\nsave.image('docs.preprocessed.Rda')\r\nload('docs.preprocessed.Rda')\r\n```\r\n\r\n\r\n### Welcome to GitHub Page\r\nThis automatic page generator is the easiest way to create beautiful pages for all of your projects. Author your page content here [using GitHub Flavored Markdown](https://guides.github.com/features/mastering-markdown/), select a template crafted by a designer, and publish. After your page is generated, you can check out the new `gh-pages` branch locally. If you’re using GitHub Desktop, simply sync your repository and you’ll see the new branch.\r\n\r\n\r\n\r\n\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}